## 7.1 Introduction
线性回归是统计学和监督机器学习的 work house。 当加上核或者其他拓展函数的时候，它也可以建模非线性关系。当高斯
输出被替换成二项式或多项式分布时，它也可以用作分类，这是接下来将要看到的。 所以值得详细学习。
## 7.2 Model specification
如同在1.4.5中讨论的， 线性回归是一个如下形式的模型
线性回归可以建模非线性关系通过将x替换成非线性函数。，即
就是是所谓的 **basis funciton expansion** 要注意到此时在参数 W 的角度， 模型依然是线性的， 所以它依然被
称为线性回归，此重要性在接下来将会清晰 一个简单的例子的是多项式基函数
我们也可以在线性回归应用于不止单输入。 比如，考虑到
## 7.3 Maximum likelyhood estimation（最小二乘）
统计模型通常的一个估计参数的方法是极大似然估计 ，定义如下
$\theta$ = argmaxlogP(D|\thata) （这是估计方法，不同的是将p换掉？ 感觉发现了不得了的东西233）
通常假设训练样本的IID （独立同分布）
。。。
与其最大化对数概率函数，我们可以对等的最小化负对数对数似然函数 即NLL
。。
NLL 公式有时很方便， 因为很多优化软件包的设计出寻找函数的最小值而不是最大值。

现在，让我们将MLE应用于线性回归， 插入上面的高斯定义
。。。（最后就是最小二乘）
RSS称为平方差和SSE， SSE/N 称为均方误差或MSE

### 7.3.3 convexity
当讨论最小二乘时，我们注意到NLL是一个具有唯一最小值的碗状。 专业术语叫 凸函数。 凸函数在机器学习中有着非常重要的位置
让我们更准确地定义一下凸函数，我们说一个集合的凸的当且仅当我们有
。。。
直观上说， 一个严格的凸函数是一个碗状也因此有一个全局最小点，就是碗的底部。 因此，二阶导数应该大于 0 
NLL是凸函数这是非常理想的， 因为这意味着我们可以找到全局最小点 这一点本书 后文也有 很多例子，但是，许多模型并不是凸的
，在这样的情况下，我们将会讨论局部最优参数估计的方法。
## 7.5 岭回归
ML的一个问题就是它会过拟合。这一部分中，我们讨论用 MAP estimati withe Gaussion prior 来改进这一问题， 为简单计，我们
假设高斯分布。

## 7.5.1 Basic idea
MLE 过拟合的原因的它是从最好你和训练集数据的角度开选择参数的 但是，如果数据由噪声，这样参数就会变得很复杂。
一个简单的例子，假设。。。对应的最小二乘系数如下
。。。
我们看到有很多正数和负数，这些平衡是使曲线“摆动”以正确的方式使它几乎完全插入数据， 但是这种情况是不合适的， 如果
我们稍稍改变一下数据，系数就会变很多。 
我们鼓励参数变得很小，这样结果就会更平滑，by 使用一个零均的高斯先验。相应的MAP估计问题将变成了
。。。（就是假设了参数趋向0）也就是等价于
。。。
这就是岭回归，也叫带惩罚最小二乘。 通常来说，加高斯先验称为L2正则或weight decay. 要注意到the offset term w0并
没有被正则，因为它只是影响函数高度而不是复杂度。通过参数和，我们确保了函数的简单的
### 7.5.4 Regularization effects of big data
正则是避免过拟合最通常的做法， 但是，另一个有效的方法-但也不是只是有效-就是使用大量数据。 很直观的说， 训练数据越多
学的越好。所以我们希望测试集误差能够随着N 的增加而降低。
## 7.6 贝叶斯线性回归
尽管岭回归是计算点估计的一个很有用的方法， 也是我们也想计算一下w 和   的全后验

