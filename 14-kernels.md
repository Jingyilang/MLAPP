## 14.1 Introduction
本书到目前为止，我们都在假设我们想要分类、聚类。或者处理的目前都可以被表示成一个固定的向量。但是， 有一些
目标， 不清楚怎么最好的表示成什么样的固定特征向量。 比如说，我们怎么表示文档或者序列？向量的长度是多少？
我们解决问题的办法是为数据定义一个生成模型， 并使用推断的潜在表示和/或模型的参数作为特征，然后将这些特征
插入到标准方法中。例如，在28章中，我们讨论了深度学习，它本质上是一种无监督的方法来学习良好的特征表示。

另一种方法是假设我们有某种方法来度量对象之间的相似性，这不需要将它们预处理成特征向量格式。比如，在比较字符串的
时候， 我们计算他们之间的距离。

本章中，我们讨论核函数集中不同的形式。然后，我们描述了一些可以纯粹用核函数计算来编写的算法。当我们不能访问
（或选择不查看）我们正在处理的对象X的“内部”时，可以使用这种方法。

## 14.2 Kernel functions
我们定义一个有两个参数的实数函数，k(x,x') R, 通常函数是对称的和非负的，所以它可以理解成一种相似性衡量的方法，但
并不需要这样做。下面我们给出几个例子。

### 14.2.1 RBF kernels
亦即高斯核

14.2.2 Kernel for comparing documents
当执行文档分类或者检索的时候， 有一种比较两个文档的方法是十分有用的，如果我们用词袋表示

## 14.3 Using kernels inside GLMs
本节中， 我们讨论一种简单的在分类和回归中使用核的方法。 随后会看到另一种方法。

### 14.3.1 kernel machines
