## 16.1 Introduction
在14和15章中，我们讨论了核方法，其为回归和分类提供了一个有效的创建非线性的方法。 尽管那已经工作
的很好了， 但是它依赖一个合适的核函数来衡量数据向量之间的相似性。经常是想到一个合适的核函数是很
困难的。 举个例子， 我们怎么定义两个图片之间的相似性？
替代方法是完全放弃内核，转而尝试从数据中直接学习有用的 ， 即我们将要创造的 **自适应基函数**， 有
如下形式：
。。。
其中  代表着第m个函数-是从数据中直接学到的。这个框架概括了我们在本章中讨论的我们模型。
通常基函数是有参数的

## 16.2 Classification and regression trees(CART)
分类与回归树，这叫做决策树（与决策理论中的决策树不同）通过递归划分输入空间，并在输入空间的每个
结果区域中定义一个局部模型来定义。这可以由树来表示

### 16.2.1 Basics
我们解释分类与回归树算法，

### 16.2.2 Growing a tree
C4.5 ID3

#### 16.2.2.1 Regression cost
#### 16.2.2.2 Classification cost

### 16.2.3 Pruning a tree （剪枝）

### 16.2.4 Pros and cons fo tress
CART 模型广受欢迎有几个原因： 易解释，容易处理离散和连续混合输入，它们对输入的单调变换不敏感
（因为分裂点是基于对数据点的排序），它们执行自动变量选择，它们对离群值比较健壮，它们很好地扩展
到大数据集，并且它们可以被修改以处理丢失的输入。
但是其也有缺点， 相比于其他的模型，CART往往预测的不是十分准确。这也部分因为这部分是由于树构造
算法的贪婪性质造成的。另一个相关问题是树的不稳定性：输入中的微小变化会导致树结构很大的影响，这是因为
树生成过程的分层特性，在频率观点中，我们说树是很高方差的估计器。我们将在下面讨论解决方案。

### 16.2.5 Random forests
一种降低估计器方差的方法是计算多个估计器的均值。比如说，我们可以爱数据的子集上训练多个不同的树。
