## 8.1 Introduction
一种建立概率分类器的方法是 to create a joint model of the form p(y,x) and then to condotion
 on x, thereby deriving p(y|x). 这就叫做生成模型。 一个替代方法是直接拟合这样的模型p(y}x), zhe 
 叫做判别模型， 这就是我们这一章讨论的。 特别的是，我们将假设这些判别模型参数是线性的。 我们将会
 看到拟合起来就很简单。 在8.6， 我们比较生成和判别模型，在后面的章节， 我们将会考虑非线性和无参数
 判别模型。
 
 ## 8.2 Model specification
 
 就像我们在1.4.6中讨论的 逻辑回归对应于下面的二分类模型：
 
 ## 8.3 Model fitting
 
 这章中，我们讨论估计逻辑回归模型参数的方法。
 
 ### 8.3.1 
 
 NLL在逻辑回归里是
NLL(W) = -\sum log []
 这这叫做交叉熵误差模型。
 也可以是下面的写法， 假设y是-1和1,而不是0，1
 。。。
 不像线性回归，我们不在写下MLE的闭式解。 取而代之的是我们需要用优化算法来计算它，正因如此，我们需要提到梯度和
 Hessian 矩阵。
 在逻辑回归中，看到梯度和Hessian (二阶偏导数矩阵)如下:
 。。。
 
 ### 8.3.2 Steepst descent (最速下降)
 
 可能最简单的无约束优化算法就是梯度下降，也是所谓的最速下降，这个可以被写作如下形式：
  \theta k+1 = \theta k - nkgk
  nk 是步长或学习率。 在梯度下降的主要问题是怎么设置学习率？ 这好像和棘手。 如果我们用一个固定的学习率，小了，收敛
  的慢，大了， 可能收敛失败。 
 
### 8.3.3 Newton's method

可以通过计算空间的曲率来获得更快的优化方法。 叫做**二阶优化方法** 主要的例子就是牛顿算法。 
