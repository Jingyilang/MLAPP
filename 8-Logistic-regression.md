## 8.1 Introduction
一种建立概率分类器的方法是 to create a joint model of the form p(y,x) and then to condotion
 on x, thereby deriving p(y|x). 这就叫做生成模型。 一个替代方法是直接拟合这样的模型p(y}x), zhe 
 叫做判别模型， 这就是我们这一章讨论的。 特别的是，我们将假设这些判别模型参数是线性的。 我们将会
 看到拟合起来就很简单。 在8.6， 我们比较生成和判别模型，在后面的章节， 我们将会考虑非线性和无参数
 判别模型。
 
 ## 8.2 Model specification
 
 就像我们在1.4.6中讨论的 逻辑回归对应于下面的二分类模型：
 
 ## 8.3 Model fitting
 
 这章中，我们讨论估计逻辑回归模型参数的方法。
 
 ### 8.3.1 
 
 NLL在逻辑回归里是
NLL(W) = -\sum log []
 这这叫做交叉熵误差模型。
 也可以是下面的写法， 假设y是-1和1,而不是0，1
 。。。
 不像线性回归，我们不在写下MLE的闭式解。 取而代之的是我们需要用优化算法来计算它，正因如此，我们需要提到梯度和
 Hessian 矩阵。
 在逻辑回归中，看到梯度和Hessian (二阶偏导数矩阵)如下:
 。。。
 
 ### 8.3.2 Steepst descent (最速下降)
 
 可能最简单的无约束优化算法就是梯度下降，也是所谓的最速下降，这个可以被写作如下形式：
  \theta k+1 = \theta k - nkgk
  nk 是步长或学习率。 在梯度下降的主要问题是怎么设置学习率？ 这好像和棘手。 如果我们用一个固定的学习率，小了，收敛
  的慢，大了， 可能收敛失败。 
 
### 8.3.3 Newton's method

可以通过计算空间的曲率来获得更快的优化方法。 叫做**二阶优化方法** 主要的例子就是牛顿算法。

## 8.6 Genarative vs discriminative classifiers

在4.2.2 章， 我们展示了。。。决策边界所以是一个a linear function of x in both cases。 但是， 许多
生成模型
这些模型更深层次的不同的他们训练的方法。 当拟合一个判别模型的时候，我们通常会最大化conditional log
likelihood logp(y|x,\theta)， 而当拟合一个生成模型的时候，我们是最大化联合对数似然函数log p(y, x|\theta)
很明显， 这会有不同的结果。
当高斯假设是正确的， 模型会比逻辑回归需要更少的训练，达到更好的效果， 但是如果高斯假设不对，逻辑回归
表现的就更好，这是因为判别模型不需要建模特征的分布。 。。。。。， 这就建议我们， 判别模型通常跟准确
因为它们的工作在某些程度上更容易。但是， 准确率并不仅仅的选择一个方法唯一的主要因素。接下来我们将会
讨论每一个方法的优点和缺点。

### 8.6.4 
。Easy to fit? 就像我们看到的， 生成模型通常更容易拟合，例如，在。。。我们膳食了通过简单的计数和
平均可以拟合一个朴素贝叶斯， 但是相反， 逻辑回归需要解决一个凸优化问题，这就要慢一点。
。Fit classes separately? 早生成模型中，我们独立估计每一类条件密度,所以我们不必再新加入类的时候再重新
训练模型。相反，在判别模型中， 所有的参数相互作用，所以加入一个新类的时候就要重新训练模型。
。Handle missing features easily? 有时一些输入并没有别发现。在生成模型中，有一个简单的解决这一问题的方法
， 在8.6.2章讨论。但是在判别模型中， 我们解决这一问题的有力方法，因为模型假设 x 总是可得的
。 Can handle unlabled training data? 还是生成好
。Symmetric in inputs and outputs?  我们
。Can handle feature preprocessing? 判别模型的一大优点就是他们允许我们预处理输入。我们可以将x替换成
(x)， 就是拓展函数。生成函数不行。
。一些生成方法，比如说朴素贝叶斯，在独立性有很强的假设，这通常不对。这可能会导致很极端的后果。生成方法，比如时候
逻辑回归，在方面就很好。
我们看到这两种模型都有赞成和反对的理由。因此，在“工具箱”中同时使用这两种方法是很有用的。
